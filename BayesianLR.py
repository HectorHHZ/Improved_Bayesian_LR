import keras
import tensorflow as tf
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np
import random

class Data_Preprocessing():
    def __init__(self, x_TrainingPath, y_TrainingPath, x_TestingPath, y_TestingPath ):
        self.x_train_path = x_TrainingPath
        self.y_train_path = y_TrainingPath
        self.x_test_path = x_TestingPath
        self.y_test_path = y_TestingPath

    def load_date(self):
        self.x_train_0 = np.loadtxt(self.x_train_path)
        self.y_train = np.loadtxt(self.y_train_path)
        self.x_test_0 = np.loadtxt(self.x_test_path)
        self.y_test = np.loadtxt(self.y_test_path)
        #return self.x_train, self.y_train, self.x_test, self.y_test

    #Transform the dataset
    def standardScale(self):
        sc = StandardScaler()
        self.x_train_1 = sc.fit_transform(self.x_train_0)
        self.x_test_1 = sc.transform(self.x_test_0)

    #PCA preprocessing for X_Train, X_Test to one dimension
    def PCA_preprocessing(self):
        pca = PCA.transform(1)
        self.x_train_2 = pca.fit_transform(self.x_train_1)
        self.x_test_2 = pca.fit_transform(self.x_test_1)
        return self.x_train_2, self.x_test_2, self.y_train, self.y_test

def GenerateX(x1, x2, x3, x4, x5, x6, x7):
    #This function is used to generate X matrix, combining all features together
    #Both X_train and X_test are generated by this function
    #Should return a Matrix
    #To be Finished
    return None

class Bayesian_Logistic_Regression():
    def __init__(self, x, y):
        self.GammaParameters = [1, 0.1]

        # initialize Gamma
        self.Gamma = random.gammavariate(self.GammaParameters[0], self.GammaParameters[1])

        # initialize theta = log beta
        self.MultiNormal_Cov = np.diag([self.Gamma] * 7)
        self.mean = (0, 0, 0, 0, 0, 0, 0)

        # initialize Beta
        self.beta = np.random.multivariate_normal(self.mean, self.MultiNormal_Cov, size=None, check_valid= 'raise')

        # X and Y are initialized and ready to use

    def Loss_function(self, x, y):
        #This function is going to calculate the loss function, which will
        #be used to determine when and where the Gradient Descent is going to end
        #Should return a float number
        #To be Finished
        return None

    def Beta_update(self, x, y):
        # This function is going to update Beta when doing Gradient Descent
        # Should return a float number
        # To be Finished
        return None

    def Theta_update(self, x, y):
        # This function is going to update Theta which Equals to log Gamma when doing Gradient Descent
        # Should return a float number
        # To be Finished
        return None



    def test(self):
        #check = np.dot(np.transpose(self.beta), self.test)
        return self.Gamma, self.MultiNormal_Cov, self.beta

def run():
    # This function is used to
    # 1. Process matrix and 2. run gradient Descent
    #

if __name__ == '__main__':
    run()










